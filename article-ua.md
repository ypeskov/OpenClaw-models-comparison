# Я протестував 8 AI-моделей як автономних агентів. Більшість завалила просте завдання з 4 кроків.

На тлі нестихаючого хайпу навколо нового агента OpenClaw, я теж вирішив подивитися на це диво. В процесі налаштування рекомендований Opus 4.5 з'їв $125 (плюс ПДВ) за 2 дні. Це доволі неприємно — якщо залишити його як є, розорить. І загалом підтверджується спостереження інших людей: OpenClaw споживає контекст кінськими дозами.

Тому було вирішено подивитися на альтернативи серед моделей з відкритими вагами (open-weight). Якимось блогом прибило нового провайдера [Synthetic.new](https://synthetic.new), який за $20/міс дає 135 запитів за 5 годин, а за $60/міс — 1350 за 5 годин.

Список моделей у них вражаючий, але з більш-менш адекватних для агентної роботи були відібрані:

- **Kimi-K2.5** (Moonshot AI)
- **Qwen3-Coder-480B** (Alibaba)
- **MiniMax M2.1** (MiniMax AI)
- **Qwen3-235B-Instruct** (Alibaba)
- **DeepSeek-V3.2** (DeepSeek)

І для порівняння — 3 моделі від Anthropic: **Claude Haiku 4.5**, **Sonnet 4.5**, **Opus 4.5**.

## Задум

Спочатку я планував провести порівняння на 5 різних видах завдань. Але це виявилося доволі виснажливою справою, і розмір посту вийшов би величезним. Тому протестував одне завдання, а решту чотири даю списком — у кого буде бажання, може провести порівняння самостійно та поділитися:

**Завдання 1: Multi-step tool calling chain (послідовні виклики)** ← протестоване

**Завдання 2: Error recovery (обробка помилок)**
Дай агенту завдання з завідомо проблемним кроком — наприклад: "Прочитай файл `data/report.csv`, порахуй середнє значення колонки `revenue`, виведи результат." Але файлу не існує.

**Завдання 3: Multilingual instruction following**
Змішаний промпт: "Напиши email англійською колезі Andrey, в якому коротко опиши результати тестування моделей. Тема листа російською. В кінці додай P.S. болгарською з побажанням гарного дня."

**Завдання 4: Structured output + reasoning**
"У мене є три API-провайдери з різними характеристиками latency, uptime та ціни. Виведи JSON з ранжуванням по best value, поясни логіку, запропонуй primary і fallback."

**Завдання 5: Multi-tool orchestration (паралельні інструменти)**
"Знайди в інтернеті бенчмарки моделі, перевір у Google Drive попередні результати тестів, і на основі обох джерел склади порівняльну таблицю."

Детальні описи завдань 2–5 із системою оцінювання — у [репозиторії на GitHub](посилання).

## Завдання 1: що саме робили моделі

Кожна модель отримувала однаковий промпт у чистій сесії (без попереднього контексту):

> Виконай наступне завдання покроково. Кожен крок потребує використання відповідного інструменту (tool call). Не пропускай кроки і не об'єднуй їх.
>
> **Крок 1:** Використай інструмент для отримання поточної погоди в місті Софія, Болгарія (координати: 42.6977, 23.3219).
>
> **Крок 2:** На основі отриманих даних, склади короткий опис погоди болгарською мовою (2–3 речення): температуру, стан, рекомендацію щодо одягу.
>
> **Крок 3:** Створи файл `weather_sofia.md` із заданою markdown-структурою (заголовок, дата, опис, дані).
>
> **Крок 4:** Надішли файл як вкладення на email. Тема та тіло листа — болгарською.

Завдання перевіряє все, що потрібно від агента: отримання даних з API, обробку, створення файлу, надсилання email із вкладенням. Чотири послідовні tool call'и, кожен залежить від попереднього.

**Система оцінювання (10 балів):**
- Правильний порядок tool calls (3)
- Коректність кожного виклику (3)
- Якість фінального результату (2)
- Ефективність та комунікація (2)

Оцінював я сам, із Claude Opus в ролі со-арбітра.

## Результати

| Модель | Бал | Час | Вартість | Email із вкладенням | Пінги від юзера |
|--------|-----|-----|----------|:---:|:---:|
| **Claude Opus 4.5** | **9.5** | 25 сек | ~$0.43 | ✅ | 0 |
| **MiniMax M2.1** | **9.5** | 45 сек | підписка | ✅ | 0 |
| **Kimi-K2.5** | **9.5** | 1:25 | підписка | ✅ | 0 |
| Claude Sonnet 4.5 | 8.5 | ~1 хв | ~$0.30 | ✅* | 0 |
| Claude Haiku 4.5 | 6.0 | 3–4 хв | ~$0.05 | ❌ | 3 |
| Qwen3-Coder-480B | 4.5 | 1:30 | підписка | ❌ | 1 |
| Qwen3-235B-Instruct | 2.5 | 10+ хв | підписка | ❌ | 6+ |
| DeepSeek-V3.2 | 0 | завис | підписка | — | 3 |

*Sonnet не зміг прикріпити файл штатними інструментами і написав Python-скрипт для SMTP. Винахідливо, але надмірно.

## Короткий розбір по моделях

**MiniMax M2.1** — найшвидша з open-weight моделей (45 сек), все виконано за один прохід. Єдина модель, яка показала дощ із грозою — і виявилася правою (про це нижче). Email із вкладенням через штатний інструмент, жодних запитань.

**Kimi-K2.5** — надійне виконання за 1:25. Акуратний перерахунок вітру з км/год у м/с. Email із вкладенням штатно. З кумедного: при спробі переключити модель на початку сесії — відмовився, бо пам'ятав інцидент, коли несанкціонована зміна моделі зламала систему.

**Claude Opus 4.5** — найшвидший з усіх (25 сек), найчистіший файл (єдиний, хто написав заголовок секції болгарською «Данни» замість російського «Данные»). Але $0.43 за завдання. Для продакшну — дорого.

**Claude Sonnet 4.5** — не зміг прикріпити файл штатними засобами, але не став питати користувача, а написав Python-скрипт для SMTP і через нього надіслав. Типовий Sonnet: якщо інструмент не працює — напишу свій. ~$0.30 за завдання.

**Claude Haiku 4.5** — написав найкращий болгарський текст з усіх («топла, многослойна облека, желателно с яке и шапка»). Але на email-кроці зламався: питав користувача про app password, надіслав Telegram-повідомлення із запитом GPG-пароля, і в підсумку засунув вміст файлу в тіло листа замість вкладення.

**Qwen3-Coder-480B** — пройшов усі кроки, але email двічі надіслав без вкладення. Після фідбеку повторив ту саму помилку. Ще заговорив болгарською в терміналі, хоча спілкування йшло російською.

**Qwen3-235B-Instruct** — найгірший UX: 10 хвилин постійного babysitting. Модель казала «секунду...», зависала, вимагала пінг. В підсумку видала файл без поля «вітер», email без вкладення, і порадила взяти парасольку при погодному коді «частково хмарно». Створювала ілюзію роботи без реального прогресу.

**DeepSeek-V3.2** — мовчки завис після отримання погодних даних. Три пінги проігноровано. Завдання довелося кинути.

## Що я зрозумів

**Бенчмарки не завжди передбачають якість агентної роботи.** DeepSeek-V3.2 та Qwen3-235B чудово виглядають на папері. На практиці один мовчки завис, інший 10 хвилин вимагав постійних пінгів. При цьому MiniMax M2.1, який у лідербордах помітно скромніший за топові моделі, впорався найкраще за сукупністю швидкості та надійності.

**Надсилання вкладення — великий зрівнювач.** 4 з 8 моделей не змогли прикріпити файл до листа. Це не проблема мислення — це проблема механіки tool calling. Моделі, які впоралися, зробили це тому що зрозуміли доступні інструменти, а не тому що вони «розумніші».

**Мовчазний збій гірший за краш.** DeepSeek завис без повідомлення про помилку. Qwen3-235B 10 хвилин казав «секунду...». Обидва варіанти гірші за явну помилку — користувач не розуміє, чекати чи здаватися. Агент, який ламається гучно, надійніший за того, що ламається мовчки.

**Швидкість важливіша, ніж здається.** MiniMax впорався за 45 секунд. Qwen3-235B витратив 10+ хвилин на гірший результат. В агентних workflow повільне виконання накопичується — якщо кожен tool call займає хвилини, завдання з 10 кроків розтягується на годину.

**У співвідношення ціна/якість є sweet spot.** Opus 4.5 був найшвидшим і найчистішим — але за $0.43 на завдання. MiniMax M2.1 видав зіставну якість на підписці з відносно безлімітними викликами за $20–60/міс. Для продакшн-агентів математика очевидна.

**Погода виявилася сюжетним поворотом.** Більшість моделей показали «частково хмарно» з Open-Meteo API. MiniMax показав «сильний дощ із грозою». Я вже збирався знизити оцінку за галюцинацію — але подивився у вікно. Весь день йшов дощ, грім був пів години тому. MiniMax виявився точнішим за всіх. Мораль: не довіряй сліпо своїм критеріям оцінки.

## Мій висновок

Для мого агента OpenClaw основною моделлю залишається **MiniMax M2.1**. Швидкий, надійний, правильно використовує інструменти і коштує в рази дешевше пропрієтарних альтернатив. **Kimi-K2.5** — сильний запасний варіант. А для завдань, де важлива максимальна якість і бюджет не обмежений — **Opus 4.5** поза конкуренцією за швидкістю та точністю.

Детальні результати по кожній моделі (з точними висновками, розбивкою оцінок та нотатками про поведінку) — на GitHub: [посилання на репозиторій]

---

*Методологія: кожна модель тестувалася в чистій сесії з однаковим промптом. Платформа — OpenClaw з доступом до API погоди, файлової системи та email-інструментів (himalaya, gog CLI). Оцінка: автор + Claude Opus 4.5 як со-арбітр. Файлова система між сесіями не очищалася — артефакти однієї моделі могли бути доступні наступній.*
