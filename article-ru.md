# Я протестировал 8 AI-моделей как автономных агентов. Большинство завалило простую задачу из 4 шагов.

На фоне неутихающего хайпа по поводу нового агента OpenClaw, я тоже решил посмотреть на это чудо. В процессе настройки рекомендованный Opus 4.5 сожрал $125 (плюс НДС) за 2 дня. Это довольно неприятно — если оставить его как есть, разорит. И в целом подтверждается наблюдение других людей: OpenClaw потребляет контекст конскими дозами.

Поэтому было решено посмотреть на альтернативы среди моделей с открытыми весами (open-weight). Каким-то блогом прибило нового провайдера [Synthetic.new](https://synthetic.new), который за $20/мес даёт 135 запросов за 5 часов, а за $60/мес — 1350 за 5 часов.

Список моделей у них внушительный, но из более-менее адекватных для агентной работы были отобраны:

- **Kimi-K2.5** (Moonshot AI)
- **Qwen3-Coder-480B** (Alibaba)
- **MiniMax M2.1** (MiniMax AI)
- **Qwen3-235B-Instruct** (Alibaba)
- **DeepSeek-V3.2** (DeepSeek)

И для сравнения — 3 модели от Anthropic: **Claude Haiku 4.5**, **Sonnet 4.5**, **Opus 4.5**.

## Задумка

Изначально я планировал провести сравнение на 5 разных видах задач. Но это оказалось довольно утомительным делом, и размер поста вышел бы огромным. Поэтому протестировал одну задачу, а остальные четыре даю списком — у кого будет желание, может провести сравнение самостоятельно и поделиться:

**Задача 1: Multi-step tool calling chain (последовательные вызовы)** ← протестирована

**Задача 2: Error recovery (обработка ошибок)**
Дай агенту задачу с заведомо проблемным шагом — например: "Прочитай файл `data/report.csv`, посчитай среднее значение колонки `revenue`, и выведи результат." Но файла не существует.

**Задача 3: Multilingual instruction following**
Смешанный промпт: "Напиши email на английском коллеге Andrey, в котором кратко опиши результаты тестирования моделей. Тема письма на русском. В конце добавь P.S. на болгарском с пожеланием хорошего дня."

**Задача 4: Structured output + reasoning**
"У меня есть три API-провайдера с разными характеристиками latency, uptime и цены. Выведи JSON с ранжированием по best value, объясни логику, предложи primary и fallback."

**Задача 5: Multi-tool orchestration (параллельные инструменты)**
"Найди в интернете бенчмарки модели, проверь в Google Drive предыдущие результаты тестов, и на основе обоих источников составь сравнительную таблицу."


## Задача 1: что именно делали модели

Каждая модель получала одинаковый промпт в чистой сессии (без предыдущего контекста):

> Выполни следующую задачу пошагово. Каждый шаг требует использования соответствующего инструмента (tool call). Не пропускай шаги и не объединяй их.
>
> **Шаг 1:** Используй инструмент для получения текущей погоды в городе София, Болгария (координаты: 42.6977, 23.3219).
>
> **Шаг 2:** На основе полученных данных, составь краткое описание погоды на болгарском языке (2–3 предложения): температуру, состояние, рекомендацию по одежде.
>
> **Шаг 3:** Создай файл `weather_sofia.md` с заданной markdown-структурой (заголовок, дата, описание, данные).
>
> **Шаг 4:** Отправь файл как вложение на email. Тема и тело письма — на болгарском.

Задача проверяет всё, что нужно от агента: получение данных из API, обработку, создание файла, отправку по email с вложением. Четыре последовательных tool call'а, каждый зависит от предыдущего.

**Система оценки (10 баллов):**
- Правильный порядок tool calls (3)
- Корректность каждого вызова (3)
- Качество финального результата (2)
- Эффективность и коммуникация (2)

Оценивал я сам, с Claude Opus в роли со-арбитра.

## Краткий разбор по моделям

**MiniMax M2.1** — самый быстрый из open-weight моделей (45 сек), всё выполнено за один проход. Единственная модель, которая показала дождь с грозой — и оказалась права (об этом ниже). Email с вложением через штатный инструмент, ноль вопросов.

**Kimi-K2.5** — надёжное выполнение за 1:25. Аккуратный пересчёт ветра из км/ч в м/с. Email с вложением штатно. Из забавного: при попытке переключить модель в начале сессии — отказался, потому что помнил инцидент, когда несанкционированная смена модели сломала систему.

**Claude Opus 4.5** — самый быстрый из всех (25 сек), самый чистый файл (единственный, кто написал заголовок секции по-болгарски «Данни» вместо русского «Данные»). Но $0.43 за задачу. Для продакшна — дорого.

**Claude Sonnet 4.5** — не смог приложить файл штатными средствами, но не стал спрашивать пользователя, а написал Python-скрипт для SMTP и через него отправил. Типичный Sonnet: если инструмент не работает — напишу свой. ~$0.30 за задачу.

**Claude Haiku 4.5** — написал лучший болгарский текст из всех («топла, многослойна облека, желателно с яке и шапка»). Но на email-шаге сломался: спрашивал пользователя про app password, отправлял Telegram-сообщение с запросом GPG-пароля, и в итоге засунул содержимое файла в тело письма вместо вложения.

**Qwen3-Coder-480B** — прошёл все шаги, но email дважды отправил без вложения. После фидбэка повторил ту же ошибку. Ещё заговорил по-болгарски в терминале, хотя общение шло на русском.

**Qwen3-235B-Instruct** — худший UX: 10 минут постоянного babysitting. Модель говорила «секунду...», зависала, требовала пинг. В итоге выдала файл без поля «ветер», email без вложения, и посоветовала взять зонт при погодном коде «частично облачно». Создавала иллюзию работы без реального прогресса.

**DeepSeek-V3.2** — молча завис после получения погодных данных. Три пинга проигнорированы. Задачу пришлось бросить.

## Результаты

| Модель | Балл | Время | Стоимость | Email с вложением | Пинки от юзера |
|--------|------|-------|-----------|:---:|:---:|
| **Claude Opus 4.5** | **9.5** | 25 сек | ~$0.43 | ✅ | 0 |
| **MiniMax M2.1** | **9.5** | 45 сек | подписка | ✅ | 0 |
| **Kimi-K2.5** | **9.5** | 1:25 | подписка | ✅ | 0 |
| Claude Sonnet 4.5 | 8.5 | ~1 мин | ~$0.30 | ✅* | 0 |
| Claude Haiku 4.5 | 6.0 | 3–4 мин | ~$0.05 | ❌ | 3 |
| Qwen3-Coder-480B | 4.5 | 1:30 | подписка | ❌ | 1 |
| Qwen3-235B-Instruct | 2.5 | 10+ мин | подписка | ❌ | 6+ |
| DeepSeek-V3.2 | 0 | завис | подписка | — | 3 |

*Sonnet не смог приложить файл штатными инструментами и написал Python-скрипт для SMTP. Изобретательно, но избыточно.

## Файлы моделей

- **Claude Opus 4.5** — [claude-opus-4.5.md](claude-opus-4.5.md)
- **Claude Sonnet 4.5** — [claude-sonnet-4.5.md](claude-sonnet-4.5.md)
- **Claude Haiku 4.5** — [claude-haiku-4.5.md](claude-haiku-4.5.md)
- **MiniMax M2.1** — [minimax-m2.1.md](minimax-m2.1.md)
- **Kimi-K2.5** — [kimi-k2.5.md](kimi-k2.5.md)
- **Qwen3-Coder-480B** — [qwen3-coder-480b.md](qwen3-coder-480b.md)
- **Qwen3-235B-Instruct** — [qwen3-235b-instruct.md](qwen3-235b-instruct.md)
- **DeepSeek-V3.2** — [deepseek-v3.2.md](deepseek-v3.2.md)

## Что я понял

**Бенчмарки не всегда предсказывают качество агентной работы.** DeepSeek-V3.2 и Qwen3-235B отлично выглядят на бумаге. На практике один молча завис, другой 10 минут требовал постоянных пинков. При этом MiniMax M2.1, который в лидербордах заметно скромнее топовых моделей, справился лучше всех по совокупности скорости и надёжности.

**Отправка вложения — великий уравнитель.** 4 из 8 моделей не смогли приложить файл к письму. Это не проблема мышления — это проблема механики tool calling. Модели, которые справились, сделали это потому что поняли доступные инструменты, а не потому что они «умнее».

**Молчаливый сбой хуже краша.** DeepSeek завис без сообщения об ошибке. Qwen3-235B 10 минут говорил «секунду...». Оба варианта хуже явной ошибки — пользователь не понимает, ждать или сдаваться. Агент, который ломается громко, надёжнее того, который ломается молча.

**Скорость важнее, чем кажется.** MiniMax справился за 45 секунд. Qwen3-235B потратил 10+ минут на худший результат. В агентных workflow медленное выполнение накапливается — если каждый tool call занимает минуты, задача из 10 шагов растягивается на час.

**У соотношения цена/качество есть sweet spot.** Opus 4.5 был самым быстрым и чистым — но за $0.43 на задачу. MiniMax M2.1 выдал сопоставимое качество на подписке с относительно безлимитными вызовами за $20–60/мес. Для продакшн-агентов математика очевидна.

**Погода оказалась сюжетным поворотом.** Большинство моделей показали «частично облачно» из Open-Meteo API. MiniMax показал «сильный дождь с грозой». Я уже собирался снизить оценку за галлюцинацию — но посмотрел в окно. Весь день шёл дождь, гром был полчаса назад. MiniMax оказался точнее всех. Мораль: не доверяй слепо своим критериям оценки.

## Мой вывод

Для моего агента OpenClaw основной моделью остаётся **MiniMax M2.1**. Быстрый, надёжный, правильно использует инструменты и стоит в разы дешевле проприетарных альтернатив. **Kimi-K2.5** — сильный запасной вариант. А для задач, где важно максимальное качество и бюджет не ограничен — **Opus 4.5** вне конкуренции по скорости и точности.

Подробные результаты по каждой модели (с точными выводами, разбивкой оценок и заметками о поведении) — на: [GitHub](https://github.com/ypeskov/OpenClaw-models-comparison)

---

*Методология: каждая модель тестировалась в чистой сессии с одинаковым промптом. Платформа — OpenClaw с доступом к API погоды, файловой системе и email-инструментам (himalaya, gog CLI). Оценка: автор + Claude Opus 4.5 как со-арбитр. Файловая система между сессиями не очищалась — артефакты одной модели могли быть доступны следующей.*
